{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOS: VITAL ARRIVAL INFO extracted\n",
      "NEWS extracted\n",
      "REGISTRATION:  POLICE STATION extracted\n",
      "FERRY TO ATHENS extracted\n",
      "LODGING extracted\n",
      "LOCAL TRANSPORT extracted\n",
      "SERVICES extracted\n",
      "EMERGENCY CONTACTS extracted\n",
      "MEDICAL extracted\n",
      "CURRENCY extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luuk/pyEnv/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "def getCitySections(city_name):\n",
    "    \n",
    "    output = dict()\n",
    "    output['name'] = city_name\n",
    "    output['sections'] = []\n",
    "    \n",
    "    base_url = \"https://refugeeinfo.eu/\" + city_name.lower() + \"/en/\"\n",
    "    \n",
    "    page_content = urllib2.urlopen(base_url).read()\n",
    "    page_soup = BeautifulSoup(page_content)\n",
    "    page_container = page_soup.body.find('div', attrs={'id':'container'})\n",
    "    \n",
    "    section_blocks = re.split(r'<div class=\"section-header[^\">]*\">', str(page_container))\n",
    "    \n",
    "    for e,block in enumerate(section_blocks[1:]):\n",
    "        soup_block = BeautifulSoup(block)\n",
    "        output['sections'].append({\n",
    "                'index': e,\n",
    "                'name': soup_block.find('h3', attrs={'class':'anchor'}).find('a').get_text(),\n",
    "                'code': soup_block.prettify(),\n",
    "                'text': soup_block.get_text()\n",
    "            })\n",
    "        print output['sections'][e]['name'], 'extracted'\n",
    "        \n",
    "    return output\n",
    "    \n",
    "\n",
    "data_kos = getCitySections('kos')\n",
    "f = open('kos.json', 'w+')\n",
    "f.write(json.dumps(data_kos, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
